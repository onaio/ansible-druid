#
# Licensed to Metamarkets Group Inc. (Metamarkets) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. Metamarkets licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Extensions specified in the load list will be loaded by Druid
# We are using local fs for deep storage - not recommended for production - use S3, HDFS, or NFS instead
# We are using local derby for the metadata store - recommended for production - use MySQL or Postgres instead

# If you specify `druid.extensions.loadList=[]`, Druid won't load any extension from file system.
# If you don't specify `druid.extensions.loadList`, Druid will load all the extensions under root extension directory.
# More info: http://druid.io/docs/latest/operations/including-extensions.html
{% if druid_extensions is defined %}
druid.extensions.loadList=[{% for extension in druid_extensions %}"{{ extension }}"{% if loop.index != loop.length %}, {% endif %}{% endfor %}]
druid.extensions.directory=/opt/druid/druid-{{ druid_version }}/extensions
{% endif %}

# Hadoop dependencies are bundled in the druid installation package
druid.extensions.hadoopDependenciesDir=/opt/druid/druid-0.10.1/hadoop-dependencies

# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties=true

# Enable Double column storage - ONLY for druid v0.11.0 or later
druid.indexing.doubleStorage=double

#
# Zookeeper
#

druid.zk.service.host={{ zk_host }}
druid.zk.paths.base=/druid

#
# Metadata storage
#

# For PostgreSQL (make sure to additionally include the Postgres extension):
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://{{ pg_host }}:5432/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password={{ druid_metadata_password }}

#
# Deep storage
#

# For S3:
druid.storage.type=s3
druid.storage.bucket={{ druid_aws_s3_bucket }}
druid.storage.baseKey={{ druid_aws_s3_baseKey }}
druid.storage.storageDirectory={{ druid_aws_s3_bucket }}/{{ druid_aws_s3_baseKey }}
druid.s3.accessKey={{ dw_aws_access_key }}
druid.s3.secretKey={{ dw_aws_secret_key }}

#
# Indexing service logs
#

# For S3:
druid.indexer.logs.type=s3
druid.indexer.logs.s3Bucket={{ druid_aws_s3_bucket }}
druid.indexer.logs.s3Prefix=druid/indexing-logs

#
# Service discovery
#

druid.selectors.indexing.serviceName=druid/overlord
druid.selectors.coordinator.serviceName=druid/coordinator

#
# Monitoring
#

{% if druid_common_monitoring_monitors %}
druid.monitoring.monitors=[{% for monitor in druid_common_monitoring_monitors %}"{{ monitor }}"{% if loop.index != loop.length %}, {% endif %}{% endfor %}]
{% endif %}
druid.emitter={{ druid_emitter }}
{% if druid_emitter_configuration %}
{% for (key, value) in druid_emitter_configuration %} 
druid.emitter.{{ key }}={{ value}}
{% endfor %}
{% endif %}

#
# JavaScript
#

druid.javascript.enabled={{ druid_javascript_enabled }}

#
# SQL
#
# Druid SQL is a built-in SQL layer and an alternative to Druid's native
# JSON-based query language, and is powered by a parser and planner based on
# Apache Calcite. Druid SQL translates SQL into native Druid queries on the
# query broker (the first node you query), which are then passed down to data
# nodes as native Druid queries. Other than the (slight) overhead of
# translating SQL on the broker, there isn't an additional performance penalty
# versus native queries.

druid.sql.enable = {{ druid_sql_enabled }}
